<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

        <script type="text/x-mathjax-config">
          MathJax.Hub.Config({"HTML-CSS" : {scale : 75}})
        </script>

        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <link href="https://fonts.googleapis.com/css?family=Cardo" rel="stylesheet" type="text/css">
        <title>Google Summer of Code - Project Blog</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
        <link rel="stylesheet" type="text/css" href="../css/code.css" />
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300" rel="stylesheet" type="text/css">
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a>Google Summer of Code</a>
            </div>
            <div id="subtitle">
                <a href="../">Project Blog</a>
            </div>
        </div>

        <div id="navbar">
            <div id="navigation">
                <a href="../">Home</a>
                <a href="../about.html">About</a>
                <a href="../contact.html">Contact</a>
                <a href="../final.html">Final Submission</a>
            </div>
        </div>

        <div id="content">

          <div id="title"><h1>Testing the Prototype</h1></div>
            <div class="info">
    Posted on June 27, 2016
    
</div>

<p>In my <a href="./2016-06-14-a-first-prototype.md">last</a> blog post, I presented a first prototype of a multi-architecture implementation of deep feedforward neural networks. While the different components of the prototype were thoroughly tested using unit tests, the next step was to integrate the prototype into TMVA in order to perform an end-to-end test of the system, which compares the classification performance of the prototype to the performance of the original implementation of deep neural networks in TMVA.</p>
<h1 id="neural-network-training">Neural Network Training</h1>
<p>So far I have not discussed how the training of the neural network prototype will be performed. In general, neural networks are trained iteratively using gradient-based optimization methods. This means in each step the weights <span class="math inline">\(\mathbf{W}^l\)</span> and bias terms <span class="math inline">\(\boldsymbol{\theta}^l\)</span> of each layer <span class="math inline">\(l\)</span> are updated based on the gradients of the loss function with respect to the weights and bias terms:</p>
\begin{align}
    \mathbf{W}^l &amp;= \mathbf{W}^l + \Delta \mathbf{W} \left ( \frac{dJ}{d\mathbf{W}^l} \right ) \\
    \boldsymbol{\theta}^l &amp;= \boldsymbol{\theta}^l + \Delta \boldsymbol{\theta} \left ( \frac{dJ}{d\boldsymbol{\theta}^l} \right ) \\
\end{align}
<p>In general, the weight and bias-term updates, <span class="math inline">\(\Delta \mathbf{W}^l\)</span> and <span class="math inline">\(\Delta\boldsymbol{\theta}^l\)</span> may depend also on the previous minimization steps that have been performed. For now, however, we will only consider the simplest gradient-based minimization method, namely <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>, where the weight updates in each step are given by</p>
\begin{align}
    \mathbf{W}^l &amp;= \mathbf{W}^l - \alpha \frac{dJ}{d\mathbf{W}^l} \\
    \boldsymbol{\theta}^l &amp;= \boldsymbol{\theta}^l - \alpha \frac{dJ}{d\boldsymbol{\theta}^l} \\
\end{align}
<p>for a given <em>learning rate</em> <span class="math inline">\(\alpha\)</span>. Using the <code>scale_add</code> function defined in the low-level <a href="./2016-06-14-a-first-prototype.md">interface</a>, a gradient descent step for a given mini-batch from the trainin set can be implemented by performing forward and backward propagation and adding the computed weights and bias gradients to the weights and biases of each layer.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp">net.Forward(input);
net.Backward(input, output);

<span class="kw">for</span> (size_t i = <span class="dv">0</span>; i &lt; net.GetDepth(); i++)
{
    <span class="kw">auto</span> &amp;layer = net.GetLayer(i);
    scale_add(layer.GetWeights(), layer.GetWeightGradients(),
                <span class="fl">1.0</span>, -fLearningRate);
    scale_add(layer.GetBiases(), layer.GetBiasGradients(),
                <span class="fl">1.0</span>, -fLearningRate);
}</code></pre></div>
<p>Note that <code>input</code> and <code>output</code> are the input events and true labels of a <em>mini batch</em> from the training set in matrix form. The mini batches are generated by shuffling the training set and splitting it up into of a given batch size. Then, a <em>single</em> minimization step is performed on each of the batches. The number of minimization steps required to traverse the complete training set is called an <strong>epoch</strong>. Convergence of the algorithm is determined by periodically monitoring the loss or error on a validation set. Convergence is assumed to be achieved when the minimum loss on the validation set has not decreased for a given number of epochs.</p>
<p>The integration of the prototype into TMVA took me the last two weeks. The reason for the delay is that I decided to also refactor the implementation of the TMVA interface after discovering some bugs and shortcomings in the original implementation of DNNs in TMVA.</p>
<h1 id="results">Results</h1>
<p>For the testing, a subset of the higgs data set described in my <a href="./2016-04-28-tmva-dnn-1.md">first</a> blog post was used. The data set consists of 9000 background and 9000 signal training events as well 1000 test events of each class. The training was performed using a very simple neural net consisting of 3 hiden layers with 100, 50 and 10 neurons with ReLU activation functions for the first two hidden layers and identity activations on the last one. The relevant TMVA configuration settings are given below:</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp">TString configString = <span class="st">&quot;!H:V&quot;</span>;
configString += <span class="st">&quot;:VarTransform=G&quot;</span>;
configString += <span class="st">&quot;:ErrorStrategy=CROSSENTROPY&quot;</span>;
configString += <span class="st">&quot;:WeightInitialization=XAVIERUNIFORM&quot;</span>;
TString layoutString = <span class="st">&quot;Layout=RELU|100,RELU|50,RELU|10,LINEAR&quot;</span>;
TString trainingString = <span class="st">&quot;TrainingStrategy=&quot;</span>
                         <span class="st">&quot;LearningRate=0.0005,&quot;</span>
                         <span class="st">&quot;Momentum=0.0,&quot;</span>
                         <span class="st">&quot;Repetitions=1,&quot;</span>
                         <span class="st">&quot;ConvergenceSteps=10,&quot;</span>
                         <span class="st">&quot;BatchSize=20,&quot;</span>
                         <span class="st">&quot;Regularization=None,&quot;</span>
                         <span class="st">&quot;TestRepetitions=15,&quot;</span>
                         <span class="st">&quot;Multithreading=False&quot;</span>;

configString += <span class="st">&quot;:&quot;</span> + layoutString + <span class="st">&quot;:&quot;</span> + trainingString;
TMVA::Types::EMVA method = TMVA::Types::kDNN;
factory-&gt;BookMethod(loader, method, <span class="st">&quot;DNN&quot;</span>, configString);</code></pre></div>
<p>The rootbook used for the tests can be found <a href="https://github.com/simonpf/rootbooks/blob/master/TMVA%20DNN.ipynb">here</a>. The training was performed using standard gradient descent with a learning rate of <span class="math inline">\(\alpha = 5 \times 10^{-4}\)</span> and 10 steps without improvement in the minimum network loss required for convergence (<code>ConvergenceSteps</code>). The trained networks are evaluated by computing the <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC curve</a> of the corresponding classifier. The results are displayed in the figure below. The black line displays the result from the prototype and the red line the result from the original implementation. As can be seen from the plot, both methods achieve nearly identical classification performance.</p>
<div class="figure">
<img src="../images/roc_test_1.png" alt="Figure 1: ROC curve of the deep neural network
prototype trained on a subset of the Higgs dataset (black). The red curve displayes the
performance of the original implementation." style="width: 500px;">
<p class="caption">
<strong>Figure 1</strong>: ROC curve of the deep neural network prototype trained on a subset of the Higgs dataset (black). The red curve displayes the performance of the original implementation.
</p>
</div>
<p>The time required for the training of the neural networks were <span class="math inline">\(231 s\)</span> and <span class="math inline">\(317 s\)</span> for the new prototype and the previous implementation, respectively. While I do not expect the reference implementation to be numerically more efficient than the previous implementation, the reduction in training time may be due differences in the implementation of the networks. The previous implementation includes a bias term only for the first layer, while the current implementation uses them on every layer. This may provide additional flexibility which allows the net to adapt to the training set in less training steps. However, this is just a suspicion and a more detailed analysis will be required to make any knowledgable statements about the performance of the two implementations.</p>
<h1 id="summary-and-outlook">Summary and Outlook</h1>
<p>With the successful integration of the neural network prototype into TMVA, an important milestone on the way towards GPU-accelerated training of neural networks in TMVA was achieved. The prototype currently implements the core functionality required to train and evaluate the neural network. Moreover, the reference implementation and the generic unit tests developed so far should considerably simplify the implementation of hardware-specific backends. The next step from here will be to implement a first GPU backend in order demonstrate the capabilities of the current design.</p>

        </div>
        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
